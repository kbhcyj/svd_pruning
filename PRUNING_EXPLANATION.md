# SVD 기반 네트워크 가지치기 (Network Pruning) 시각화

## 1. 프로젝트 개요
이 프로젝트는 딥러닝 모델 경량화 기술 중 하나인 **네트워크 가지치기(Network Pruning)**의 원리를 기하학적으로 시각화하여 이해하는 것을 목표로 합니다.
특히, 선형대수학의 **특이값 분해(Singular Value Decomposition, SVD)**를 활용하여 가중치 행렬(Weight Matrix)의 정보량을 분석하고, 중요도가 낮은 성분을 제거했을 때 데이터 변환이 어떻게 달라지는지를 직관적으로 보여줍니다.

## 2. 네트워크 가지치기 (Network Pruning) 배경
네트워크 가지치기는 신경망의 파라미터 중 결과에 미치는 영향이 적은(중요하지 않은) 가중치를 제거하여 모델의 크기를 줄이고 연산 속도를 높이는 기법입니다.

### 왜 SVD인가?
완전 연결 계층(Fully Connected Layer)의 가중치 행렬 $W$는 입력 벡터 $x$를 출력 벡터 $y$로 선형 변환($y = Wx$)합니다.
SVD를 통해 행렬 $W$를 분해하면, 이 변환이 **회전(Rotation) -> 스케일링(Scaling) -> 회전(Rotation)**의 결합임을 알 수 있습니다.
이때 **스케일링(Scaling)**을 담당하는 **특이값(Singular Values)**의 크기는 해당 축 방향으로의 정보 보존량을 나타냅니다.

## 3. 코드 구현 및 작동 원리 (`test.py`)

이 프로젝트는 2차원 공간에서의 변환을 예시로 다음과 같은 과정을 수행합니다.

### 3.1 입력 데이터 (Input Space)
- **단위 원 (Unit Circle)**: 모든 방향으로의 단위 벡터들을 입력 데이터로 사용합니다. 이는 입력 공간의 모든 방향성을 대변합니다.

### 3.2 원본 변환 (Full Rank)
- **가중치 행렬 $W$**: 2x2 행렬로 설정되어 있으며, 입력을 선형 변환합니다.
- **결과**: 단위 원은 **타원(Ellipse)** 형태로 변환됩니다. 타원의 장축(Major Axis)과 단축(Minor Axis)은 행렬 $W$가 데이터를 어느 방향으로 얼마나 늘리고 줄이는지를 나타냅니다.

### 3.3 SVD와 가지치기 (Low-Rank Approximation)
코드에서는 `torch.linalg.svd`를 사용하여 $W$를 $U \Sigma V^T$로 분해합니다.
- **가지치기 전략**: $\Sigma$ (특이값 대각 행렬)에서 **가장 작은 특이값을 0으로 만듭니다.**
- **의미**: 기여도가 가장 낮은 차원(축)의 정보를 삭제하는 것입니다. 이를 **Low-Rank Approximation**이라고 합니다.

### 3.4 가지치기 후 변환 (Rank-1 Approximation)
- **결과**: 가장 작은 특이값이 제거되었으므로, 정보가 한 차원으로 붕괴(collapse)됩니다.
- **시각적 효과**: 2차원 평면상의 **타원**이 1차원 **직선(Line)**으로 납작해집니다. 이는 모델이 덜 중요한 정보를 버리고, 가장 중요한 주성분(Principal Component) 방향의 정보만 남겼음을 의미합니다.

## 4. 결론 및 시사점
이 시각화는 가지치기가 단순히 "연결을 끊는 것"을 넘어, **행렬의 랭크(Rank)를 낮추어 모델이 표현하는 부분 공간(Subspace)을 압축하는 과정**임을 보여줍니다.
- **장점**: 모델 용량 감소, 연산 가속.
- **단점**: 작은 특이값이라도 중요한 정보를 담고 있을 경우 정보 손실(타원이 직선이 됨)로 인한 정확도 하락 가능성.

따라서 실제 딥러닝 모델 가지치기에서는 적절한 임계값(Threshold)을 설정하거나, 재학습(Fine-tuning)을 통해 손실된 정보를 복구하는 과정이 필요합니다.

